> **ALPHA**:
> This is a new service â€“ your [feedback](https://github.com/govuk-digital-backbone/aiengineeringlab/discussions) will help us to improve it.
# Delivery manager assessment questionnaire

## Why we're asking for this information

This questionnaire helps us understand your team's context so that we can tailor the right support for adopting AI coding assistants. There's no right or wrong answers as we're gathering information to design an approach that works for your specific situation.

**What happens next:** your responses, along with questionnaires from your technical lead and team engineers, help us to determine the most effective adoption approach for your team. This might include customised training, specific guardrails, or additional hands-on support.

**Time needed:** approximately 10 to 15 minutes

**Confidentiality:** your responses are used only to design your team's adoption plan. No individual team data is shared beyond the programme team.

---

## About your team

### 1. Team composition
How many engineers are in your team?
- [ ] 1-3 engineers
- [ ] 4-6 engineers
- [ ] 7-10 engineers
- [ ] 11+ engineers

### 2. Engineering roles
Which engineering roles are in your team? (Select all that apply)
- [ ] Software engineers
- [ ] Data engineers
- [ ] DevOps/Platform engineers
- [ ] Test engineers
- [ ] Other (please specify): _________________

### 3. Team stability
How long has your current team been working together?
- [ ] Less than 3 months
- [ ] 3 to 6 months
- [ ] 6 to 12 months
- [ ] More than 12 months

### 4. Experience levels
What's the mix of experience in your team?
- [ ] Mostly junior (0 to 2 years experience)
- [ ] Mix of junior and mid-level (0 to 5 years)
- [ ] Mostly mid-level (3 to 7 years experience)
- [ ] Mix of mid and senior (3 to 10+ years)
- [ ] Mostly senior (7+ years experience)

---

## What your team works on

### 5. Primary work type
What type of work does your team primarily do? (Select one)
- [ ] Greenfield development (building new services from scratch)
- [ ] Application development (enhancing existing services)
- [ ] Legacy system maintenance (supporting older systems)
- [ ] Support work (L1/L2/L3 incident response and troubleshooting)
- [ ] Data projects (analytics, pipelines, integration)
- [ ] Infrastructure/platform work (cloud, DevOps, tooling)
- [ ] Mixed (multiple types regularly)

### 6. User-facing impact
Who are the primary users of what your team builds?
- [ ] Citizens (public-facing services)
- [ ] Internal government users (staff-facing tools)
- [ ] Other government departments (cross-government services)
- [ ] Mix of the above

### 7. System criticality
How would you describe the criticality of the systems your team works on?
- [ ] Critical national infrastructure (any downtime is significant)
- [ ] High importance (downtime affects many users or key services)
- [ ] Moderate importance (downtime is disruptive but manageable)
- [ ] Lower criticality (can tolerate some disruption)

---

## Delivery approach and pressures

### 8. Current delivery methodology
How does your team work? (Select all that apply)
- [ ] Agile/Scrum with regular sprints
- [ ] Kanban with continuous flow
- [ ] Waterfall or stage-gate approach
- [ ] Ad-hoc or reactive (responding to urgent requests)
- [ ] Hybrid approach
- [ ] Other (please specify): _________________

### 9. Delivery pressures
What are your team's main delivery pressures right now? (Select all that apply)
- [ ] Fixed deadline for major release/milestone
- [ ] Ongoing BAU (business as usual) support taking most capacity
- [ ] Technical debt requiring significant remediation
- [ ] Resource constraints (not enough people)
- [ ] Rapidly changing requirements
- [ ] Multiple competing priorities
- [ ] None - we have stable, manageable workload
- [ ] Other (please specify): _________________

### 10. Upcoming deadlines
Do you have any significant delivery milestones in the next 3 months?
- [ ] Yes, critical deadline we must hit
- [ ] Yes, important but some flexibility
- [ ] No major deadlines
- [ ] Uncertain - requirements still emerging

If yes, briefly describe: _________________

---

## Quality standards and practices

### 11. Code review practices
How does code get reviewed before it goes live?
- [ ] Formal peer review required for all changes
- [ ] Peer review happens for most changes
- [ ] Reviews happen occasionally or for significant changes only
- [ ] No formal review process
- [ ] Varies by engineer or urgency

### 12. Testing approach
What testing practices does your team use? (Select all that apply)
- [ ] Automated unit tests
- [ ] Automated integration tests
- [ ] Automated end-to-end tests
- [ ] Manual testing by engineers
- [ ] Manual testing by dedicated test team
- [ ] User acceptance testing
- [ ] Limited or no formal testing
- [ ] Other (please specify): _________________

### 13. Quality gates
What gates or checks exist before code reaches production? (Select all that apply)
- [ ] Automated test suite must pass
- [ ] Security scanning (SAST/DAST)
- [ ] Code quality checks (linting, complexity analysis)
- [ ] Peer review approval
- [ ] Technical lead or architect approval
- [ ] Deployment follows documented process
- [ ] None - engineers deploy directly
- [ ] Other (please specify): _________________

### 14. Technical debt
How would you characterise your team's technical debt situation?
- [ ] Low - codebase is modern and maintainable
- [ ] Moderate - some legacy issues but manageable
- [ ] High - significant legacy constraints affecting delivery
- [ ] Very high - technical debt dominates our work

---

## AI and governance context

### 15. Current AI tool usage
Do engineers in your team currently use any AI coding assistant tools?
- [ ] Yes, officially sanctioned by organisation
- [ ] Yes, but unofficially (shadow IT)
- [ ] No, not aware of any usage
- [ ] No, explicitly prohibited

If yes, specify which tools (select all that apply):
- [ ] GitHub Copilot
- [ ] Claude Code
- [ ] Google Antigravity
- [ ] Cursor
- [ ] Windsurf
- [ ] Devin
- [ ] Amazon Q
- [ ] Amazon Kiro
- [ ] ChatGPT/Claude for coding support
- [ ] Other (please specify): _________________

### 16. AI usage policy
Does your organisation have a clear policy on using AI tools for software development?
- [ ] Yes, clear written policy exists and is communicated
- [ ] Yes, policy exists but not widely known
- [ ] Informal guidance exists
- [ ] No policy exists
- [ ] Unsure

### 17. Data sensitivity
Does your team work with data that has specific handling restrictions? (Select all that apply)
- [ ] OFFICIAL-SENSITIVE data
- [ ] Personal data (GDPR)
- [ ] Commercial or procurement-sensitive data
- [ ] Security or intelligence data
- [ ] No particularly sensitive data
- [ ] Unsure what restrictions apply

### 18. Security constraints
What security constraints does your team work within? (Select all that apply)
- [ ] Cannot use cloud-based AI tools (data egress concerns)
- [ ] Must use specific approved tools only
- [ ] Restricted internet access for development machines
- [ ] Must work within secure environments (e.g., PSN, GSI)
- [ ] Standard corporate network with internet access
- [ ] Unsure about specific constraints

---

## Change and learning context

### 19. Recent changes
Has your team experienced significant changes recently? (Select all that apply)
- [ ] New team members joined
- [ ] Team members left
- [ ] Change in leadership or reporting
- [ ] New technology or platform adoption
- [ ] Change in working practices or methodology
- [ ] Organisational restructure
- [ ] No significant recent changes

### 20. Learning culture
How does your team approach learning new tools and practices?
- [ ] Enthusiastic early adopters - we seek out new approaches
- [ ] Pragmatic - we'll try new things if they solve problems
- [ ] Cautious - we need proven benefits before changing
- [ ] Resistant - we have established ways that work
- [ ] Mixed - some enthusiastic, some resistant

### 21. Time for learning
Realistically, how much time can engineers dedicate to learning new tools over the next month?
- [ ] 5+ hours per week (significant capacity)
- [ ] 2-4 hours per week (moderate capacity)
- [ ] 1 hour per week (limited capacity)
- [ ] Less than 1 hour per week (very limited)
- [ ] Essentially none - delivery pressures too high

---

## Support needs

### 22. Main concerns
What concerns you most about introducing AI coding assistants to your team? (Select up to 3)
- [ ] Data security and information leakage
- [ ] Code quality might decrease
- [ ] Engineers might become over-reliant on AI
- [ ] Time to learn during busy delivery period
- [ ] Compliance with organisational policies
- [ ] Integration with existing tools and workflows
- [ ] Cost vs benefit unclear
- [ ] Team resistance to change
- [ ] Unsure how to measure success
- [ ] Other (please specify): _________________

### 23. Success indicators
Describe how you would know this adoption has been successful for your team.

_________________

### 24. Additional context
Is there anything else about your team's context that would help us design the right support approach? (Optional)

_________________


**Thank you for completing this questionnaire.**

Your assessment lead will review your responses alongside the technical lead and individual engineer questionnaires to design your team's adoption approach.

If you have any questions, contact [Assessment Lead contact details].
