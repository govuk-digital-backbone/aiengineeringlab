> **ALPHA**
> This is a new service. Your [feedback](https://github.com/govuk-digital-backbone/aiengineeringlab/discussions) will help us to improve it.

# Delivery manager assessment questionnaire

## Why we are asking for this information

This questionnaire helps us understand your team's context so we can tailor the right support for adopting AI coding assistants. There are no right or wrong answers. We are gathering information to design an approach that works for your specific situation.

## What happens next

Your responses, along with questionnaires from your technical lead and team engineers, help us determine the most effective adoption approach for your team. This might include customised training, specific guardrails or additional hands-on support.

You will need approximately 10 to 15 minutes to complete this questionnaire.

Your responses are confidential and will only be used to design your team's adoption plan. We do not share individual team data beyond the programme team.

## About your team

### 1. Team composition
How many engineers are in your team?
- [ ] 1 to 3 engineers
- [ ] 4 to 6 engineers
- [ ] 7 to 10 engineers
- [ ] 11 and more engineers

### 2. Engineering roles
Which engineering roles are in your team? Select all that apply.
- [ ] Software engineers
- [ ] Data engineers
- [ ] DevOps and platform engineers
- [ ] Test engineers
- [ ] Other (specify below)

If other, specify: _________________

### 3. Team stability
How long has your current team been working together?
- [ ] Less than 3 months
- [ ] 3 to 6 months
- [ ] 6 to 12 months
- [ ] More than 12 months

### 4. Experience levels
What is the mix of experience in your team?
- [ ] Mostly junior (0 to 2 years experience)
- [ ] Mix of junior and mid-level (0 to 5 years)
- [ ] Mostly mid-level (3 to 7 years experience)
- [ ] Mix of mid and senior (3 to 10 and more years)
- [ ] Mostly senior (7 and more years experience)

## What your team works on

### 5. Primary work type
What type of work does your team primarily do? Select one.
- [ ] Greenfield development (building new services from scratch)
- [ ] Application development (enhancing existing services)
- [ ] Legacy system maintenance (supporting older systems)
- [ ] Support work (L1, L2 and L3 incident response and troubleshooting)
- [ ] Data projects (analytics, pipelines, integration)
- [ ] Infrastructure and platform work (cloud, DevOps, tooling)
- [ ] Mixed (multiple types regularly)

### 6. User-facing impact
Who are the primary users of what your team builds?
- [ ] Citizens (public-facing services)
- [ ] Internal government users (staff-facing tools)
- [ ] Other government departments (cross-government services)
- [ ] Mix of the above

### 7. System criticality
How would you describe the criticality of the systems your team works on?
- [ ] Critical national infrastructure (any downtime is significant)
- [ ] High importance (downtime affects many users or important services)
- [ ] Moderate importance (downtime is disruptive but manageable)
- [ ] Lower criticality (can tolerate some disruption)

## Delivery approach and pressures

### 8. Current delivery methodology
How does your team work? Select all that apply.
- [ ] Agile or scrum with regular sprints
- [ ] Kanban with continuous flow
- [ ] Waterfall or stage-gate approach
- [ ] Ad-hoc or reactive (responding to urgent requests)
- [ ] Hybrid approach
- [ ] Other (specify below)

If other, specify: _________________

### 9. Delivery pressures
What are your team's main delivery pressures right now? Select all that apply.
- [ ] Fixed deadline for major release or milestone
- [ ] Ongoing business as usual support taking most capacity
- [ ] Technical debt requiring significant remediation
- [ ] Resource constraints (not enough people)
- [ ] Rapidly changing requirements
- [ ] Multiple competing priorities
- [ ] None - we have stable, manageable workload
- [ ] Other (specify below)

If other, specify: _________________

### 10. Upcoming deadlines
Do you have any significant delivery milestones in the next 3 months?
- [ ] Yes, critical deadline we must hit
- [ ] Yes, important but some flexibility
- [ ] No major deadlines
- [ ] Uncertain - requirements still emerging

If yes, briefly describe: _________________

## Quality standards and practices

### 11. Code review practices
How does code get reviewed before it goes live?
- [ ] Formal peer review required for all changes
- [ ] Peer review happens for most changes
- [ ] Reviews happen occasionally or for significant changes only
- [ ] No formal review process
- [ ] Varies by engineer or urgency

### 12. Testing approach
What testing practices does your team use? Select all that apply.
- [ ] Automated unit tests
- [ ] Automated integration tests
- [ ] Automated end-to-end tests
- [ ] Manual testing by engineers
- [ ] Manual testing by dedicated test team
- [ ] User acceptance testing
- [ ] Limited or no formal testing
- [ ] Other (specify below)

If other, specify: _________________

### 13. Quality gates
What gates or checks exist before code reaches production? Select all that apply.
- [ ] Automated test suite must pass
- [ ] Security scanning (Static application security testing (SAST) and Dynamic application security testing (DAST))
- [ ] Code quality checks (linting, complexity analysis)
- [ ] Peer review approval
- [ ] Technical lead or architect approval
- [ ] Deployment follows documented process
- [ ] None - engineers deploy directly
- [ ] Other (specify below)

If other, specify: _________________

### 14. Technical debt
How would you describe your team's technical debt situation?
- [ ] Low - codebase is modern and maintainable
- [ ] Moderate - some legacy issues but manageable
- [ ] High - significant legacy constraints affecting delivery
- [ ] Very high - technical debt dominates our work

## AI and governance context

### 15. Current AI tool usage
Do engineers in your team currently use any AI coding assistant tools?
- [ ] Yes, officially sanctioned by organisation
- [ ] Yes, but unofficially (shadow IT)
- [ ] No, not aware of any usage
- [ ] No, explicitly prohibited

If yes, specify which tools. Select all that apply.
- [ ] GitHub Copilot
- [ ] Claude Code
- [ ] Google Antigravity
- [ ] Cursor
- [ ] Windsurf
- [ ] Devin
- [ ] Amazon Q
- [ ] Amazon Kiro
- [ ] ChatGPT or Claude for coding support
- [ ] Other (specify below)

If other, specify: _________________

### 16. AI usage policy
Does your organisation have a clear policy on using AI tools for software development?
- [ ] Yes, clear written policy exists and is communicated
- [ ] Yes, policy exists but not widely known
- [ ] Informal guidance exists
- [ ] No policy exists
- [ ] Unsure

### 17. Data sensitivity
Does your team work with data that has specific handling restrictions? Select all that apply.
- [ ] OFFICIAL-SENSITIVE data
- [ ] Personal data (GDPR)
- [ ] Commercial or procurement-sensitive data
- [ ] Security or intelligence data
- [ ] No particularly sensitive data
- [ ] Unsure what restrictions apply

### 18. Security constraints
What security constraints does your team work within? Select all that apply.
- [ ] Cannot use cloud-based AI tools (data egress concerns)
- [ ] Must use specific approved tools only
- [ ] Restricted internet access for development machines
- [ ] Must work within secure environments (for example Public services network (PSN), Government secure intranet (GSI))
- [ ] Standard corporate network with internet access
- [ ] Unsure about specific constraints

## Change and learning context

### 19. Recent changes
Has your team experienced significant changes recently? Select all that apply.
- [ ] New team members joined
- [ ] Team members left
- [ ] Change in leadership or reporting
- [ ] New technology or platform adoption
- [ ] Change in working practices or methodology
- [ ] Organisational restructure
- [ ] No significant recent changes

### 20. Learning culture
How does your team approach learning new tools and practices?
- [ ] Enthusiastic early adopters - we seek out new approaches
- [ ] Pragmatic - we will try new things if they solve problems
- [ ] Cautious - we need proven benefits before changing
- [ ] Resistant - we have established ways that work
- [ ] Mixed - some enthusiastic, some resistant

### 21. Time for learning
Realistically, how much time can engineers dedicate to learning new tools over the next month?
- [ ] 5 and more hours per week (significant capacity)
- [ ] 2 to 4 hours per week (moderate capacity)
- [ ] 1 hour per week (limited capacity)
- [ ] Less than 1 hour per week (very limited)
- [ ] Essentially none - delivery pressures too high

## Support needs

### 22. Main concerns
What concerns you most about introducing AI coding assistants to your team? Select up to 3.
- [ ] Data security and information leakage
- [ ] Code quality might decrease
- [ ] Engineers might become over-reliant on AI
- [ ] Time to learn during busy delivery period
- [ ] Compliance with organisational policies
- [ ] Integration with existing tools and workflows
- [ ] Cost versus benefit unclear
- [ ] Team resistance to change
- [ ] Unsure how to measure success
- [ ] Other (specify below)

If other, specify: _________________

### 23. Success indicators
How would you know this adoption has been successful for your team? (open text)

_________________

### 24. Additional context
Is there anything else about your team's context that would help us design the right support approach? (open text - optional)

_________________

## Thank you for completing this questionnaire

Your assessment lead will review your responses alongside the technical lead and individual engineer questionnaires to design your team's adoption approach.

If you have any questions, contact the assessment lead.